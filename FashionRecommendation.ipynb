{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Myntra FashionRecommendation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot7zCq57yb8u"
      },
      "source": [
        "# #reference: \n",
        "# #https://forums.fast.ai/t/how-to-find-similar-images-based-on-final-embedding-layer/16903\n",
        "\n",
        "# Initial Setup\n",
        "\n",
        "!pip install fastai\n",
        "!pip install annoy\n",
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbjhASzNzCOT"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "from fastai.vision import *\n",
        "from fastai.metrics import accuracy, top_k_accuracy\n",
        "from annoy import AnnoyIndex\n",
        "import zipfile\n",
        "import time\n",
        "from google.colab import drive\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuValdAtzJ22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRlgRa96zyav"
      },
      "source": [
        "# DeepFashion Dataset\n",
        "Dataset Link: http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html\n",
        "\n",
        "We are using v1 of the data, with 24K+ images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7nquRak0M7k"
      },
      "source": [
        "# collect meta data\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pWnFiNlNGTVloLUk'\n",
        "output = 'list_category_cloth.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pTGNoWkhZeVpzbFk'\n",
        "output = 'list_category_img.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=0B7EVK8r0v71pdS1FMlNreEwtc1E'\n",
        "output = 'list_eval_partition.txt'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgBVRVjPzZSo"
      },
      "source": [
        "# collect the images\n",
        "root_path = './'\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/img.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(root_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtFnkErb0esk"
      },
      "source": [
        "category_list = []\n",
        "image_path_list = []\n",
        "data_type_list = []\n",
        "# category names\n",
        "with open('/content/drive/MyDrive/list_category_cloth.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i > 1:\n",
        "            category_list.append(line.split(' ')[0])\n",
        "\n",
        "# category map\n",
        "with open('/content/drive/MyDrive/list_category_img.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i > 1:\n",
        "            image_path_list.append([word.strip() for word in line.split(' ') if len(word) > 0])\n",
        "\n",
        "\n",
        "# train, valid, test\n",
        "with open('/content/drive/MyDrive/list_eval_partition.txt', 'r') as f:\n",
        "    for i, line in enumerate(f.readlines()):\n",
        "        if i > 1:\n",
        "            data_type_list.append([word.strip() for word in line.split(' ') if len(word) > 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKp8b3NB0roM"
      },
      "source": [
        "data_df = pd.DataFrame(image_path_list, columns=['image_path', 'category_number'])\n",
        "data_df['category_number'] = data_df['category_number'].astype(int)\n",
        "data_df = data_df.merge(pd.DataFrame(data_type_list, columns=['image_path', 'dataset_type']), on='image_path')\n",
        "data_df['category'] = data_df['category_number'].apply(lambda x: category_list[int(x) - 1])\n",
        "data_df = data_df.drop('category_number', axis=1)\n",
        "data_df.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLmyYR5v0yqW"
      },
      "source": [
        "len(data_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K06aGsk80z-j"
      },
      "source": [
        "data_df[['image_path','dataset_type']].groupby('dataset_type').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwqr2OFe05EF"
      },
      "source": [
        "len(data_df.category.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2rUbvKK0_J1"
      },
      "source": [
        "data_df[['image_path','category']].groupby('category').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWXi7PqG1MWa"
      },
      "source": [
        "# Convert images to embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYC0sQKE1LOW"
      },
      "source": [
        "train_image_list = ImageList.from_df(df=data_df, path=root_path, cols='image_path').split_by_idxs(\n",
        "    (data_df[data_df['dataset_type']=='train'].index),\n",
        "    (data_df[data_df['dataset_type']=='val'].index)).label_from_df(cols='category')\n",
        "test_image_list = ImageList.from_df(df=data_df[data_df['dataset_type'] == 'test'], path=root_path, cols='image_path')\n",
        "\n",
        "data = train_image_list.transform(get_transforms(), size=224).databunch(bs=128).normalize(imagenet_stats)\n",
        "data.add_test(test_image_list)\n",
        "data.show_batch(rows=3, figsize=(8,8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TFRJgVV2yGx"
      },
      "source": [
        "# Transfer Learning from ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24D4G6CJ1kR_"
      },
      "source": [
        "# ResNet 18/50\n",
        "\n",
        "def train_model(data, pretrained_model, model_metrics):\n",
        "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
        "    learner.model = torch.nn.DataParallel(learner.model)\n",
        "    learner.lr_find()\n",
        "    learner.recorder.plot(suggestion=True)\n",
        "    return learner\n",
        "\n",
        "pretrained_model = models.resnet18 \n",
        "\n",
        "\n",
        "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
        "learner = train_model(data, pretrained_model, model_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69rJeFG42-1B"
      },
      "source": [
        "def find_appropriate_lr(model:Learner, lr_diff:int = 15, loss_threshold:float = .05, adjust_value:float = 1, plot:bool = False) -> float:\n",
        "    #Learning Rate Finder\n",
        "    model.lr_find()\n",
        "    \n",
        "    #Computing loss values and their corresponding gradients, and lr values\n",
        "    losses = np.array(model.recorder.losses)\n",
        "    assert(lr_diff < len(losses))\n",
        "    loss_grad = np.gradient(losses)\n",
        "    lrs = model.recorder.lrs\n",
        "    \n",
        "    #Searching for index in gradients where loss is lowest before the loss spike\n",
        "    #Initializing right and left idx using the lr_diff as a spacing unit\n",
        "    #Local min lr set as -1 to signify if threshold is too low\n",
        "    r_idx = -1\n",
        "    l_idx = r_idx - lr_diff\n",
        "    while (l_idx >= -len(losses)) and (abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold):\n",
        "        local_min_lr = lrs[l_idx]\n",
        "        r_idx -= 1\n",
        "        l_idx -= 1\n",
        "\n",
        "    lr_to_use = local_min_lr * adjust_value\n",
        "    \n",
        "    if plot:\n",
        "        # plots the gradients of the losses in respect to the learning rate change\n",
        "        plt.plot(loss_grad)\n",
        "        plt.plot(len(losses)+l_idx, loss_grad[l_idx],markersize=10,marker='o',color='red')\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlabel(\"Index of LRs\")\n",
        "        plt.show()\n",
        "\n",
        "        plt.plot(np.log10(lrs), losses)\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xlabel(\"Log 10 Transform of Learning Rate\")\n",
        "        loss_coord = np.interp(np.log10(lr_to_use), np.log10(lrs), losses)\n",
        "        plt.plot(np.log10(lr_to_use), loss_coord, markersize=10,marker='o',color='red')\n",
        "        plt.show()\n",
        "        \n",
        "    return lr_to_use\n",
        "find_appropriate_lr(learner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92kxthmG3oEc"
      },
      "source": [
        "learner.fit_one_cycle(5, max_lr=1e-02)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mv74g7S3rg5"
      },
      "source": [
        "# Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nmfhsV73wI5"
      },
      "source": [
        "interp = ClassificationInterpretation.from_learner(learner)\n",
        "interp.plot_top_losses(9, largest=False, figsize=(15,11), heatmap_thresh=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTXjvZUd30CR"
      },
      "source": [
        "interp.plot_confusion_matrix(figsize=(12,12), dpi=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qso7KDUQ35Hc"
      },
      "source": [
        "# saving the model\n",
        "drive.mount('/content/gdrive')\n",
        "learner.save('/content/gdrive/My Drive/resnet18-fashion')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXbCLoCQ4Nt1"
      },
      "source": [
        "# Fastai hooks to retrieve image embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwZD93k94Z3T"
      },
      "source": [
        "class SaveFeatures():\n",
        "    features=None\n",
        "    def __init__(self, m): \n",
        "        self.hook = m.register_forward_hook(self.hook_fn)\n",
        "        self.features = None\n",
        "    def hook_fn(self, module, input, output): \n",
        "        out = output.detach().cpu().numpy()\n",
        "        if isinstance(self.features, type(None)):\n",
        "            self.features = out\n",
        "        else:\n",
        "            self.features = np.row_stack((self.features, out))\n",
        "    def remove(self): \n",
        "        self.hook.remove()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSQjq_7q4eo-"
      },
      "source": [
        "# loading the trained model\n",
        "def load_learner(data, pretrained_model, model_metrics, model_path):\n",
        "    learner = cnn_learner(data, pretrained_model, metrics=model_metrics)\n",
        "    learner.model = torch.nn.DataParallel(learner.model)\n",
        "    learner = learner.load(model_path)\n",
        "    return learner\n",
        "\n",
        "pretrained_model = models.resnet18 \n",
        "\n",
        "\n",
        "model_metrics = [accuracy, partial(top_k_accuracy, k=1), partial(top_k_accuracy, k=5)]\n",
        "# if gdrive not mounted:\n",
        "drive.mount('/content/gdrive') \n",
        "\n",
        "\n",
        "model_path = \"/content/gdrive/My Drive/resnet18-fashion\"\n",
        "learner = load_learner(data, pretrained_model, model_metrics, model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcBAq-va4pue"
      },
      "source": [
        "saved_features = SaveFeatures(learner.model.module[1][4])\n",
        "_= learner.get_preds(data.train_ds)\n",
        "_= learner.get_preds(DatasetType.Valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QcfCch_4yKY"
      },
      "source": [
        "# getting the embeddings from trained model\n",
        "img_path = [str(x) for x in (list(data.train_ds.items) +list(data.valid_ds.items))]\n",
        "label = [data.classes[x] for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
        "label_id = [x for x in (list(data.train_ds.y.items) +list(data.valid_ds.y.items))]\n",
        "data_df_ouput = pd.DataFrame({'img_path': img_path, 'label': label, 'label_id': label_id})\n",
        "data_df_ouput['embeddings'] = np.array(saved_features.features).tolist()\n",
        "data_df_ouput"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VM6oTbwC5A0o"
      },
      "source": [
        "# Approximate Nearest Neighbors to obtain most similar images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-3FnlmE5NDI"
      },
      "source": [
        "# Using Spotify's Annoy\n",
        "def get_similar_images_annoy(annoy_tree, img_index, number_of_items=12):\n",
        "    start = time.time()\n",
        "    img_id, img_label  = data_df_ouput.iloc[img_index, [0, 1]]\n",
        "    similar_img_ids = annoy_tree.get_nns_by_item(img_index, number_of_items+1)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    # ignoring first item as it is always target image\n",
        "    return img_id, img_label, data_df_ouput.iloc[similar_img_ids[1:]] \n",
        "\n",
        "\n",
        "# for images similar to centroid \n",
        "def get_similar_images_annoy_centroid(annoy_tree, vector_value, number_of_items=12):\n",
        "    start = time.time()\n",
        "    similar_img_ids = annoy_tree.get_nns_by_vector(vector_value, number_of_items+1)\n",
        "    end = time.time()\n",
        "    print(f'{(end - start) * 1000} ms')\n",
        "    \n",
        "    return data_df_ouput.iloc[similar_img_ids[1:]] \n",
        "\n",
        "\n",
        "def show_similar_images(similar_images_df, fig_size=[10,10], hide_labels=True):\n",
        "    if hide_labels:\n",
        "        category_list = []\n",
        "        for i in range(len(similar_images_df)):\n",
        "            \n",
        "            category_list.append(CategoryList(similar_images_df['label_id'].values*0,\n",
        "                                              [''] * len(similar_images_df)).get(i))\n",
        "    else:\n",
        "        category_list = [learner.data.train_ds.y.reconstruct(y) for y in similar_images_df['label_id']]\n",
        "    return learner.data.show_xys([open_image(img_id) for img_id in similar_images_df['img_path']],\n",
        "                                category_list, figsize=fig_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCtomb3D5QY3"
      },
      "source": [
        "# more tree = better approximation\n",
        "ntree = 100\n",
        "#\"angular\", \"euclidean\", \"manhattan\", \"hamming\", or \"dot\"\n",
        "metric_choice = 'angular'\n",
        "\n",
        "annoy_tree = AnnoyIndex(len(data_df_ouput['embeddings'][0]), metric=metric_choice)\n",
        "\n",
        "for i, vector in enumerate(data_df_ouput['embeddings']):\n",
        "    annoy_tree.add_item(i, vector)\n",
        "_  = annoy_tree.build(ntree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olw4DC865mxq"
      },
      "source": [
        "# Embeddings Centroid Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dye53jfG5oWu"
      },
      "source": [
        "def centroid_embedding(outfit_embedding_list):\n",
        "    number_of_outfits = outfit_embedding_list.shape[0]\n",
        "    length_of_embedding = outfit_embedding_list.shape[1]\n",
        "    centroid = []\n",
        "    for i in range(length_of_embedding):\n",
        "        centroid.append(np.sum(outfit_embedding_list[:, i])/number_of_outfits)\n",
        "    return centroid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ajJozmk56hA"
      },
      "source": [
        "# Demo\n",
        "Urban Lifestyle\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kINFydJX5__w"
      },
      "source": [
        "# urban \n",
        "\n",
        "outfit_img_ids = [50374, 226654, 60186, 56384, 46254, 212771, 118639, 33918, 228773, 46332, 118173, 146836]\n",
        "outfit_embedding_list = []\n",
        "for img_index in outfit_img_ids:\n",
        "    outfit_embedding_list.append(data_df_ouput.iloc[img_index, 3])\n",
        "\n",
        "outfit_embedding_list = np.array(outfit_embedding_list)\n",
        "outfit_centroid_embedding = centroid_embedding(outfit_embedding_list)\n",
        "outfits_selected = data_df_ouput.iloc[outfit_img_ids] \n",
        "\n",
        "similar_images_df = get_similar_images_annoy_centroid(annoy_tree, outfit_centroid_embedding, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl449N6G6FZh"
      },
      "source": [
        "# Display selected Images\n",
        "show_similar_images(outfits_selected, fig_size=[15,15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdu3W05t6WRi"
      },
      "source": [
        "# Display Recommended Images\n",
        "show_similar_images(similar_images_df, fig_size=[20,20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StiwZtM06iMH"
      },
      "source": [
        "Flower Theme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP5jEFYL6kEw"
      },
      "source": [
        "# flower theme\n",
        "\n",
        "outfit_img_ids = [171787, 168315, 194847,244804, 153507, 166507, 172396, 14497, 200965, 162629,241277, 126155]\n",
        "outfit_embedding_list = []\n",
        "for img_index in outfit_img_ids:\n",
        "    outfit_embedding_list.append(data_df_ouput.iloc[img_index, 3])\n",
        "\n",
        "outfit_embedding_list = np.array(outfit_embedding_list)\n",
        "outfit_centroid_embedding = centroid_embedding(outfit_embedding_list)\n",
        "outfits_selected = data_df_ouput.iloc[outfit_img_ids] \n",
        "\n",
        "similar_images_df = get_similar_images_annoy_centroid(annoy_tree, outfit_centroid_embedding, 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q74Dysw66nug"
      },
      "source": [
        "# Display selected Images\n",
        "show_similar_images(outfits_selected, fig_size=[15,15])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWKkjXfR6rGu"
      },
      "source": [
        "# Display Recommended Images\n",
        "show_similar_images(similar_images_df, fig_size=[20,20])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}